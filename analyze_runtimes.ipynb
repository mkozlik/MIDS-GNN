{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "import torch_geometric.utils as tg_utils\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from MIDS_dataset import MIDSDataset, MIDSLabelsDataset, MIDSProbabilitiesDataset\n",
    "from MIDS_script import generate_model\n",
    "from my_graphs_dataset import GraphDataset\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(root):\n",
    "    # Set up parameters.\n",
    "    seed = 42\n",
    "    selected_graph_sizes = {\n",
    "        # \"26-50_mix_100\": -1,\n",
    "        \"03-25_mix_750\": -1\n",
    "    }\n",
    "    split = (0.6, 0.2)\n",
    "    batch_size = 1\n",
    "\n",
    "    # Get the dataset.\n",
    "    loader = GraphDataset(selection=selected_graph_sizes, seed=seed)\n",
    "    prob_dataset = MIDSProbabilitiesDataset(root / \"Dataset\", loader)\n",
    "    labels_dataset = MIDSLabelsDataset(root / \"Dataset\", loader, selected_extra_feature=\"predicted_probability\")\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    prob_dataset, perm = prob_dataset.shuffle(return_perm=True)\n",
    "    labels_dataset = labels_dataset.index_select(perm) # type: ignore\n",
    "    assert isinstance(prob_dataset, MIDSDataset)\n",
    "    assert isinstance(labels_dataset, MIDSDataset)\n",
    "\n",
    "    # Flexible dataset splitting. Can be split to train/test or train/val/test.\n",
    "    if isinstance(split, tuple):\n",
    "        train_size, val_size = split\n",
    "        train_size = round(train_size * len(prob_dataset))\n",
    "        val_size = round(val_size * len(prob_dataset))\n",
    "    else:\n",
    "        train_size = round(split * len(prob_dataset))\n",
    "        val_size = len(prob_dataset) - train_size\n",
    "\n",
    "    test_probs = prob_dataset[train_size + val_size:]\n",
    "    test_labels = labels_dataset[train_size + val_size:]\n",
    "\n",
    "    # Batch and load data.\n",
    "    prob_loader = DataLoader(test_probs, batch_size, shuffle=False, pin_memory=True)  # type: ignore\n",
    "    labels_loader = DataLoader(test_labels, batch_size, shuffle=False, pin_memory=True)  # type: ignore\n",
    "    prob_data = [test_batch for test_batch in prob_loader]\n",
    "    labels_data = [test_batch for test_batch in labels_loader]\n",
    "\n",
    "    return prob_data, labels_data, labels_dataset.num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions for testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utilities.mids_utils import check_MIDS\n",
    "\n",
    "def run_GNN(root, prob_data, label_data, num_features):\n",
    "    device = \"cpu\"  # \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Load the probability model.\n",
    "    prob_model = torch.load(root / \"Models\" / \"prob_model_best.pth\")\n",
    "    prob_model.to(device)\n",
    "    prob_model.eval()\n",
    "\n",
    "    # Load the best results csv file.\n",
    "    load_path = root / \"Results\" / \"labels_all_best.csv\"\n",
    "    best_df = pd.read_csv(load_path, index_col=0)\n",
    "    selected_df = best_df[best_df['selected_extra_feature'] == 'predicted_probability']\n",
    "\n",
    "    # Perform the experiment over models and data examples.\n",
    "    records = []\n",
    "    for _, row in tqdm(selected_df.iterrows(), total=len(selected_df)):\n",
    "        # Load the model.\n",
    "        model_kwargs = {}\n",
    "        if row[\"architecture\"] == \"GIN\":\n",
    "            model_kwargs = {\"train_eps\": True}\n",
    "        elif row[\"architecture\"] == \"GAT\":\n",
    "            model_kwargs = {\"v2\": True}\n",
    "\n",
    "        label_model = generate_model(\n",
    "            row[\"architecture\"],\n",
    "            num_features,\n",
    "            row[\"hidden_channels\"],\n",
    "            row[\"gnn_layers\"],\n",
    "            act=row[\"activation\"],\n",
    "            jk=row[\"jk\"] if row[\"jk\"] != \"none\" else None,\n",
    "            **model_kwargs\n",
    "        )\n",
    "        saved_model_dict = torch.load(root / \"Models\" / f\"{row['id']}_best_model.pth\", weights_only=False)\n",
    "        label_model.load_state_dict(saved_model_dict[\"model_state_dict\"])\n",
    "        label_model.to(device)\n",
    "        label_model.eval()\n",
    "\n",
    "        # Calculate the execution time on each data example\n",
    "        for i in trange(len(prob_data), leave=False, desc=row[\"architecture\"]):\n",
    "            start = time.perf_counter()\n",
    "            # First, predict probabilities.\n",
    "            example = prob_data[i].to(device)\n",
    "            out = prob_model(example.x, example.edge_index)\n",
    "\n",
    "            # Second, predict labels.\n",
    "            example = label_data[i].to(device)\n",
    "            out = label_model(example.x, example.edge_index)\n",
    "            end = time.perf_counter()\n",
    "\n",
    "            # Third, check the MIDS.\n",
    "            pred = torch.where(out > 0, 1.0, 0.0).numpy()\n",
    "            A = tg_utils.to_dense_adj(example.edge_index).squeeze().cpu().numpy()\n",
    "            try:\n",
    "                correct = int(check_MIDS(A, pred, sum(example.y[:, 0].numpy())))\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            records.append({\n",
    "                \"model\": row[\"architecture\"],\n",
    "                \"num_nodes\": example.num_nodes,\n",
    "                \"num_edges\": example.num_edges,\n",
    "                \"correct\": correct,\n",
    "                \"execution_time\": (end - start) * 1000\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utilities.baselines import optimize_Gurobi\n",
    "\n",
    "def run_Gurobi(label_data):\n",
    "    records = []\n",
    "\n",
    "    for i in trange(len(label_data)):\n",
    "        nx_graph = tg_utils.to_networkx(label_data[i], to_undirected=True)\n",
    "        sol, execution_time, details = optimize_Gurobi(nx_graph, \"MIDS\", goal=\"D\", outputFlag=0, single_cpu=False)\n",
    "        correct = 0 if len(sol) == 0 else 1\n",
    "        records.append({\n",
    "            \"model\": \"Gurobi\",\n",
    "            \"num_nodes\": nx_graph.number_of_nodes(),\n",
    "            \"num_edges\": nx_graph.number_of_edges(),\n",
    "            \"correct\": correct,\n",
    "            \"execution_time\": execution_time\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utilities.mids_utils import find_MIDS\n",
    "\n",
    "def run_BK(label_data):\n",
    "    records = []\n",
    "\n",
    "    for i in trange(len(label_data)):\n",
    "        nx_graph = tg_utils.to_networkx(label_data[i], to_undirected=True)\n",
    "        start = time.perf_counter()\n",
    "        sol = find_MIDS(nx_graph)\n",
    "        end = time.perf_counter()\n",
    "        correct = 0 if len(sol) == 0 else 1\n",
    "        records.append({\n",
    "            \"model\": \"Bron-Kerbosch\",\n",
    "            \"num_nodes\": nx_graph.number_of_nodes(),\n",
    "            \"num_edges\": nx_graph.number_of_edges(),\n",
    "            \"correct\": correct,\n",
    "            \"execution_time\": (end - start) * 1000\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utilities.baselines import ILPS\n",
    "from Utilities.mids_utils import check_MIDS\n",
    "import networkx as nx\n",
    "\n",
    "def run_ILPS(label_data):\n",
    "    records = []\n",
    "\n",
    "    for i in trange(1000):\n",
    "        nx_graph = tg_utils.to_networkx(label_data[i], to_undirected=True)\n",
    "        start = time.perf_counter()\n",
    "        sol = ILPS(nx_graph)\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        sol = [1 if x in sol else 0 for x in range(nx_graph.number_of_nodes())]\n",
    "        A = nx.to_numpy_array(nx_graph)\n",
    "        correct = int(check_MIDS(A, sol, int(label_data[i].y[:,0].numpy().sum())))\n",
    "        records.append({\n",
    "            \"model\": \"ILPS\",\n",
    "            \"num_nodes\": nx_graph.number_of_nodes(),\n",
    "            \"num_edges\": nx_graph.number_of_edges(),\n",
    "            \"correct\": correct,\n",
    "            \"execution_time\": (end - start) * 1000\n",
    "        })\n",
    "\n",
    "    print(f\"Correct: {correct}/{len(label_data)}\")\n",
    "\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions for processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization & Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.validators.scatter.marker import SymbolValidator\n",
    "import plotly.io as pio\n",
    "\n",
    "def make_runtime_heatmap(by_nodesize_results, datapath):\n",
    "    # Pivot the data to create a matrix for the heatmap\n",
    "    heatmap_data = by_nodesize_results.pivot_table(\n",
    "        index=\"num_nodes\", columns=[\"Method\"], values=\"avg\"\n",
    "    )\n",
    "\n",
    "    # Add hover data for \"avg\" and \"std\"\n",
    "    hover_data = []\n",
    "    for model in by_nodesize_results[\"Method\"].unique():\n",
    "        for size in by_nodesize_results[\"num_nodes\"].unique():\n",
    "            avg_time = by_nodesize_results[\n",
    "                (by_nodesize_results[\"Method\"] == model) & (by_nodesize_results[\"num_nodes\"] == size)\n",
    "            ][\"avg\"].values[0]\n",
    "            std_dev = by_nodesize_results[\n",
    "                (by_nodesize_results[\"Method\"] == model) & (by_nodesize_results[\"num_nodes\"] == size)\n",
    "            ][\"std\"].values[0]\n",
    "            hover_data.append((size, model, f\"Size: {size}<br>{avg_time:.3f} ± {std_dev:.3f}\"))\n",
    "\n",
    "    hover_df = pd.DataFrame(hover_data, columns=[\"num_nodes\", \"Method\", \"hover\"])\n",
    "    hover_matrix = hover_df.pivot(index=\"num_nodes\", columns=\"Method\", values=\"hover\")\n",
    "    hover_matrix = hover_matrix.loc[:, list(heatmap_data.columns)]\n",
    "\n",
    "    # Plot the heatmap\n",
    "    fig = px.imshow(\n",
    "        np.log10(heatmap_data),\n",
    "        title=\"Average Execution Time by Node Size and Model\",\n",
    "        labels={\"color\": \"Execution Time (ms)\"},\n",
    "        color_continuous_scale=px.colors.sequential.Viridis[::-1]\n",
    "    )\n",
    "\n",
    "    # Add custom hover template\n",
    "    fig.update_traces(text=hover_matrix.values, hovertemplate=\"%{text}\")\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(tickmode='array', tickvals=heatmap_data.columns, ticktext=heatmap_data.columns),\n",
    "        yaxis=dict(tickmode='array', tickvals=heatmap_data.index, ticktext=heatmap_data.index)\n",
    "    )\n",
    "    min_z = np.log10(heatmap_data.min(axis=None))\n",
    "    max_z = np.log10(heatmap_data.max(axis=None))\n",
    "    fig.update_layout(coloraxis_colorbar=dict(\n",
    "        tickvals=np.linspace(min_z, max_z, num=6),\n",
    "        ticktext=[f\"{10**x:.3f}\" for x in np.linspace(min_z, max_z, num=6)]\n",
    "    ))\n",
    "\n",
    "    fig.show()\n",
    "    fig.write_html(datapath / \"runtimes.html\")\n",
    "\n",
    "\n",
    "def make_accuracy_heatmap(by_nodesize_results, datapath):\n",
    "    acc_heatmap = by_nodesize_results.pivot_table(\n",
    "        index=\"num_nodes\", columns=[\"Method\"], values=\"acc\"\n",
    "    )\n",
    "\n",
    "    # Plot the heatmap\n",
    "    fig = px.imshow(\n",
    "        acc_heatmap,\n",
    "        title=\"Average Accuracy by Node Size and Model\",\n",
    "        labels={\"color\": \"Accuracy (%)\"},\n",
    "        color_continuous_scale=px.colors.sequential.Viridis[::-1]\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    fig.write_html(datapath / \"accuracies.html\")\n",
    "\n",
    "\n",
    "def make_runtime_lineplot(by_nodesize_results, datapath):\n",
    "    # Create a lineplot with error bars\n",
    "    fig = px.line(\n",
    "        by_nodesize_results,\n",
    "        x=\"num_nodes\",\n",
    "        y=\"avg\",\n",
    "        color=\"Method\",\n",
    "        symbol=\"Method\",\n",
    "        symbol_sequence=SymbolValidator().values[2::12],\n",
    "        # error_y=by_nodesize_results[\"max\"] - by_nodesize_results[\"avg\"],\n",
    "        # error_y_minus=by_nodesize_results[\"avg\"] - by_nodesize_results[\"min\"],\n",
    "        # title=\"Execution Time by Node Size and Model\",\n",
    "        labels={\"avg\": \"Average Execution Time (ms)\", \"num_nodes\": \"Number of Nodes\"},\n",
    "        log_y=True\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=8))\n",
    "\n",
    "    # # Create a grouped bar plot with standard deviation\n",
    "    # fig = px.bar(\n",
    "    #     by_nodesize_results,\n",
    "    #     x=\"num_nodes\",\n",
    "    #     y=\"avg\",\n",
    "    #     color=\"Method\",\n",
    "    #     pattern_shape=\"Method\",\n",
    "    #     error_y=by_nodesize_results[\"max\"] - by_nodesize_results[\"avg\"],\n",
    "    #     error_y_minus=by_nodesize_results[\"avg\"] - by_nodesize_results[\"min\"],\n",
    "    #     barmode=\"group\",\n",
    "    #     title=\"Execution Time by Node Size and Model\",\n",
    "    #     labels={\"avg\": \"Average Execution Time (ms)\", \"num_nodes\": \"Number of Nodes\"},\n",
    "    #     log_y=True\n",
    "    # )\n",
    "    # fig.update_traces(error_y=dict(visible=True, symmetric=False, width=0, thickness=1))\n",
    "    # fig.update_layout(xaxis_type='category')\n",
    "\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        ),\n",
    "        font=dict(size=16),\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    fig.write_html(datapath / \"runtimes_line.html\")\n",
    "    # fig.write_image(\"runtimes_line.pdf\")\n",
    "    pio.write_image(fig, datapath / \"runtimes_line.pdf\", format=\"pdf\", width=1000, height=500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_latex_tables(overall_results, by_nodesize_results):\n",
    "    model_order = [\"Gurobi\", \"Bron-Kerbosch\", \"MLP\", \"GCN\", \"GIN\", \"GraphSAGE\", \"GAT\", \"GATLinNet\"]\n",
    "    latex_options = dict(position=\"htb\", escape=False)\n",
    "\n",
    "    overall_results = overall_results.reset_index()\n",
    "\n",
    "    ## 1. Overall execution time results\n",
    "    # Combine avg and std columns into a single column with ± sign\n",
    "    overall_results[\"Execution Time (ms)\"] = overall_results[\"avg\"].round(3).astype(str) + \" ± \" + overall_results[\"std\"].round(3).astype(str)\n",
    "    overall_results = overall_results.drop(columns=[\"avg\", \"std\"])\n",
    "    overall_results = overall_results.rename(columns={\"acc\": \"Accuracy (%)\"})\n",
    "\n",
    "    # Sort rows with a specified order\n",
    "    overall_results[\"Method\"] = pd.Categorical(overall_results[\"Method\"], categories=model_order, ordered=True)\n",
    "    overall_results = overall_results.sort_values(\"Method\")\n",
    "\n",
    "    table1 = overall_results.drop(columns=[\"Accuracy (%)\"])\n",
    "    caption = r\"Combined average and standard deviation of execution times on the test set containing graphs with \\textbf{insert here} nodes.\"\n",
    "    overall_times_latex = table1.to_latex(label=\"tab:total_runtime\", index=False, header=True, column_format=\"l|l\", caption=caption, **latex_options)\n",
    "\n",
    "    ## 2. Execution time results by node size\n",
    "    # Combine avg and std columns into a single column with ± sign, and rename columns\n",
    "    by_nodesize_results[\"Execution Time (ms)\"] = by_nodesize_results[\"avg\"].round(3).astype(str) + \" ± \" + by_nodesize_results[\"std\"].round(3).astype(str)\n",
    "    by_nodesize_results = by_nodesize_results.drop(columns=[\"avg\", \"std\"])\n",
    "    by_nodesize_results = by_nodesize_results.rename(columns={\"acc\": \"Accuracy (%)\", \"num_nodes\": \"Number of nodes\"})\n",
    "\n",
    "    # Pivot the dataframe to create the desired table\n",
    "    nodesize_pivot = by_nodesize_results.pivot_table(\n",
    "        index='Number of nodes',\n",
    "        columns=['Method'],\n",
    "        values='Execution Time (ms)',\n",
    "        aggfunc=\"first\"\n",
    "    )\n",
    "    nodesize_pivot = nodesize_pivot[model_order]\n",
    "\n",
    "    # Add a row with values of overall execution times from the first table\n",
    "    overall_times = overall_results.set_index(\"Method\")[\"Execution Time (ms)\"]\n",
    "    nodesize_pivot.loc[\"Overall\"] = overall_times\n",
    "\n",
    "    caption = r\"Average and standard deviation of execution times on the test set containing graphs with \\textbf{insert here} nodes per method and graph size.\"\n",
    "    nodesize_times_latex = nodesize_pivot.to_latex(label=\"tab:size_runtime\", column_format=\"c\" * (len(model_order) + 1) , caption=caption, **latex_options)\n",
    "\n",
    "    ## 3. Accuracy results by node size\n",
    "    # Pivot the dataframe to create the desired table\n",
    "    nodesize_pivot = by_nodesize_results.pivot_table(\n",
    "        index='Number of nodes',\n",
    "        columns=['Method'],\n",
    "        values='Accuracy (%)',\n",
    "        aggfunc=\"first\"\n",
    "    )\n",
    "    nodesize_pivot = nodesize_pivot[model_order]\n",
    "\n",
    "    # Add a row with values of overall accuracies from the first table\n",
    "    overall_accuracies = overall_results.set_index(\"Method\")[\"Accuracy (%)\"]\n",
    "    nodesize_pivot.loc[\"Overall\"] = overall_accuracies\n",
    "\n",
    "    caption = r\"Average accuracy on the test set containing graphs with \\textbf{insert here} nodes per method and graph size.\"\n",
    "    nodesize_accuracy_latex = nodesize_pivot.to_latex(label=\"tab:size_accuracy\", column_format=\"c\" * (len(model_order) + 1), float_format=\"%.2f\", caption=caption, **latex_options)\n",
    "\n",
    "    return overall_times_latex, nodesize_times_latex, nodesize_accuracy_latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(results):\n",
    "    # Average and standard deviation of execution time for each model over all data examples.\n",
    "    overall = results.groupby(\"model\")\n",
    "    overall_avg = overall[\"execution_time\"].mean()\n",
    "    overall_std = overall[\"execution_time\"].std()\n",
    "    overall_min = overall[\"execution_time\"].min() # quantile(0.25)\n",
    "    overall_max = overall[\"execution_time\"].max() # quantile(0.75)\n",
    "    overall_acc = overall[\"correct\"].mean() * 100\n",
    "\n",
    "    # Average and standard deviation of execution time for each model over all data examples with the same number of nodes.\n",
    "    by_nodesize = results.groupby([\"model\", \"num_nodes\"])\n",
    "    by_nodesize_avg = by_nodesize[\"execution_time\"].mean()\n",
    "    by_nodesize_std = by_nodesize[\"execution_time\"].std()\n",
    "    by_nodesize_min = by_nodesize[\"execution_time\"].min() # quantile(0.25)\n",
    "    by_nodesize_max = by_nodesize[\"execution_time\"].max() # quantile(0.75)\n",
    "    by_nodesize_acc = by_nodesize[\"correct\"].mean() * 100\n",
    "\n",
    "    return (pd.DataFrame({\"avg\": overall_avg, \"std\": overall_std, \"min\": overall_min, \"max\": overall_max, \"acc\": overall_acc}),\n",
    "            pd.DataFrame({\"avg\": by_nodesize_avg, \"std\": by_nodesize_std, \"min\": by_nodesize_min, \"max\": by_nodesize_max, \"acc\": by_nodesize_acc}))\n",
    "\n",
    "\n",
    "def final_analysis(overall_results, by_nodesize_results):\n",
    "    model_order = [\"Gurobi\", \"Bron-Kerbosch\", \"MLP\", \"GCN\", \"GIN\", \"GraphSAGE\", \"GAT\", \"GATLinNet\"]\n",
    "\n",
    "    # Join overall results\n",
    "    overall_results = pd.concat(overall_results, axis=0, keys=overall_results.keys())\n",
    "    overall_results = overall_results.droplevel(0)\n",
    "    overall_results[\"acc\"] = overall_results[\"acc\"].round(2)\n",
    "\n",
    "    # Join by_nodesize results\n",
    "    by_nodesize_results = pd.concat(by_nodesize_results, axis=0, keys=by_nodesize_results.keys())\n",
    "\n",
    "    # Reset index for easier manipulation\n",
    "    by_nodesize_results = by_nodesize_results.reset_index()\n",
    "    by_nodesize_results = by_nodesize_results.drop(columns=\"level_0\")\n",
    "    by_nodesize_results[\"acc\"] = by_nodesize_results[\"acc\"].round(2)\n",
    "\n",
    "    # Rename the model to Method\n",
    "    overall_results = overall_results.rename_axis(\"Method\")\n",
    "    by_nodesize_results = by_nodesize_results.rename(columns={\"model\": \"Method\"})\n",
    "\n",
    "    # Rename GAT row to GATv2\n",
    "\n",
    "    # Sort index by model order\n",
    "    overall_results = overall_results.reindex(model_order)\n",
    "    print(overall_results)\n",
    "\n",
    "    by_nodesize_results[\"Method\"] = pd.Categorical(by_nodesize_results[\"Method\"], categories=model_order, ordered=True)\n",
    "    by_nodesize_results[\"Method\"] = by_nodesize_results[\"Method\"].replace({\"GAT\": \"GATv2\"})\n",
    "    by_nodesize_results = by_nodesize_results.sort_values([\"Method\", \"num_nodes\"])\n",
    "    print(by_nodesize_results)\n",
    "\n",
    "    # Make a heatmap of runtimes\n",
    "    make_runtime_heatmap(by_nodesize_results)\n",
    "\n",
    "    # Make a heatmap of accuracies\n",
    "    make_accuracy_heatmap(by_nodesize_results)\n",
    "\n",
    "    # Make a line plot of runtimes\n",
    "    make_runtime_lineplot(by_nodesize_results)\n",
    "\n",
    "    # Make a line plot of accuracies\n",
    "\n",
    "    return overall_results, by_nodesize_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = pathlib.Path().cwd()  # For Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset.\n",
    "prob_data, label_data, num_features = load_dataset(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_results = dict()\n",
    "by_nodesize_results = dict()\n",
    "\n",
    "# Experiment for GNN models\n",
    "print(\"Running GNN models\")\n",
    "GNN_results = run_GNN(root, prob_data, label_data, num_features)\n",
    "overall_results[\"GNN\"], by_nodesize_results[\"GNN\"] = process_results(GNN_results)\n",
    "\n",
    "# Experiment for Gurobi\n",
    "print(\"Running Gurobi\")\n",
    "Gurobi_results = run_Gurobi(label_data)\n",
    "overall_results[\"Gurobi\"], by_nodesize_results[\"Gurobi\"] = process_results(Gurobi_results)\n",
    "\n",
    "# Repeat the experiment for Bron-Kerbosch\n",
    "print(\"Running Bron-Kerbosch\")\n",
    "BK_results = run_BK(label_data)\n",
    "overall_results[\"BK\"], by_nodesize_results[\"BK\"] = process_results(BK_results)\n",
    "\n",
    "# Repeat the experiment for ILPS\n",
    "# print(\"Running ILPS\")\n",
    "# ILPS_results = run_ILPS(label_data)\n",
    "# overall_results[\"ILPS\"], by_nodesize_results[\"ILPS\"] = process_results(ILPS_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "with open(root / \"Results\" / \"results_small.pkl\", \"wb\") as f:\n",
    "    pickle.dump((overall_results, by_nodesize_results), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_files = [\n",
    "    \"results_large.pkl\",\n",
    "    \"results_small.pkl\"\n",
    "]\n",
    "\n",
    "overall_results = dict()\n",
    "by_nodesize_results = dict()\n",
    "\n",
    "for file in pickle_files:\n",
    "    with open(root / \"Results\" / file, \"rb\") as f:\n",
    "        overall, by_nodesize = pickle.load(f)\n",
    "        for key, value in overall.items():\n",
    "            if key in overall_results:\n",
    "                overall_results[key] = pd.concat([overall_results[key], value])\n",
    "            else:\n",
    "                overall_results[key] = value\n",
    "        for key, value in by_nodesize.items():\n",
    "            if key in by_nodesize_results:\n",
    "                by_nodesize_results[key] = pd.concat([by_nodesize_results[key], value])\n",
    "            else:\n",
    "                by_nodesize_results[key] = value\n",
    "\n",
    "for key, value in overall_results.items():\n",
    "    overall_results[key] = value.groupby(\"model\").agg({\"avg\": \"mean\", \"std\": \"mean\", \"min\": \"min\", \"max\": \"max\", \"acc\": \"mean\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(root / \"Results\" / \"results_all.pkl\", \"wb\") as f:\n",
    "    pickle.dump((overall_results, by_nodesize_results), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(root / \"Results\" / \"results_all.pkl\", \"rb\") as f:\n",
    "    overall_results, by_nodesize_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the results\n",
    "overall_concat, by_nodesize_concat = final_analysis(overall_results, by_nodesize_results)\n",
    "\n",
    "# Make a latex table\n",
    "latex_tables = make_latex_tables(overall_concat, by_nodesize_concat)\n",
    "for table in latex_tables:\n",
    "    print(table)\n",
    "    print()\n",
    "\n",
    "# Save the results\n",
    "overall_concat.to_csv(root / \"Results\" / \"overall_execution_time.csv\")\n",
    "by_nodesize_concat.to_csv(root / \"Results\" / \"by_nodesize_execution_time.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
