{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "!pip install torch torchvision torchaudio\n",
    "\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "#!pip install torch-sparse #-f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "#!pip install torch-scatter #-f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "#!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
    "#!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
    "\n",
    "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "#!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "!pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "\n",
    "!pip install torch-geometric\n",
    "!pip install codetiming\n",
    "!pip install wandb\n",
    "!pip install plotly\n",
    "!pip install line-profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import codetiming\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "print(torch.__version__)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Loaded torch. Using *{device}* device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '/home/jovyan/MIDS-GNN/pyg_dataset.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from my_graphs_dataset import GraphDataset\n",
    "from pyg_dataset import MIDSdataset\n",
    "\n",
    "# Set up dataset.\n",
    "selected_graph_sizes = {\n",
    "    #3: -1,\n",
    "    #4: -1,\n",
    "    #5: -1,\n",
    "    #6: -1,\n",
    "    #7: -1,\n",
    "    8: -1,\n",
    "    9:  10000,\n",
    "    #10: 1000,\n",
    "    #11: 1000,\n",
    "    #12: 1000,\n",
    "    #13: 1000,\n",
    "    #14: 1000,\n",
    "    #15: 1000,\n",
    "    #20: 2000,\n",
    "    #30: 10000,\n",
    "}\n",
    "dataset_config = {\n",
    "    \"name\": \"MIDSdataset\",\n",
    "    \"selected_graphs\": str(selected_graph_sizes),\n",
    "    \"split\": 0.8,\n",
    "    \"batch_size\": 1,\n",
    "}\n",
    "\n",
    "# Load the dataset.\n",
    "root = pathlib.Path().cwd() / \"Dataset\"  # In Jupyter, the working directory is the notebook folder.\n",
    "graphs_loader = GraphDataset(selection=selected_graph_sizes)\n",
    "dataset = MIDSdataset(root, graphs_loader)\n",
    "\n",
    "# General information\n",
    "print()\n",
    "print(f\"Dataset: {dataset}:\")\n",
    "print(\"====================\")\n",
    "print(f\"Number of graphs: {len(dataset)}\")\n",
    "print(f\"Number of features: {dataset.num_features}\")\n",
    "\n",
    "# Store information about the dataset.\n",
    "dataset_config.update({\"num_graphs\": len(dataset)})#, \"features\": dataset.feature_functions})\n",
    "\n",
    "# Shuffle and split the dataset.\n",
    "torch.manual_seed(seed := 42)\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "train_size = round(dataset_config[\"split\"] * len(dataset))\n",
    "train_dataset = dataset[:train_size]\n",
    "if len(dataset) - train_size > 0:\n",
    "    test_dataset = dataset[train_size:]\n",
    "else:\n",
    "    test_dataset = train_dataset\n",
    "test_size = len(test_dataset)\n",
    "\n",
    "print()\n",
    "print(f\"Number of training graphs: {len(train_dataset)}\")\n",
    "print(f\"Number of test graphs: {len(test_dataset)}\")\n",
    "\n",
    "# Batch and load data.\n",
    "batch_size = int(np.ceil(dataset_config[\"batch_size\"] * len(train_dataset)))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=min(test_size, batch_size), shuffle=False)\n",
    "\n",
    "# If the whole dataset fits in memory, we can use the following lines to get a single large batch.\n",
    "train_batch = next(iter(train_loader))\n",
    "test_batch = next(iter(test_loader))\n",
    "\n",
    "train_data_obj = train_batch if train_loader.batch_size == train_size else train_loader\n",
    "test_data_obj = test_batch if test_loader.batch_size == test_size else test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(train_dataset.num_features, hidden_channels, heads=4)\n",
    "        self.lin1 = torch.nn.Linear(train_dataset.num_features, 4 * hidden_channels)\n",
    "        self.conv2 = GATConv(4 * hidden_channels, hidden_channels, heads=4)\n",
    "        self.lin2 = torch.nn.Linear(4 * hidden_channels, 4 * hidden_channels)\n",
    "        self.conv3 = GATConv(4 * hidden_channels, hidden_channels, heads=4)\n",
    "        self.lin3 = torch.nn.Linear(4 * hidden_channels, 4 * hidden_channels)\n",
    "        self.conv5 = GATConv(4 * hidden_channels, hidden_channels, heads=4)\n",
    "        self.lin5 = torch.nn.Linear(4 * hidden_channels, 4 * hidden_channels)\n",
    "        self.conv4 = GATConv(4 * hidden_channels, 1, heads=6,\n",
    "                             concat=False)\n",
    "        self.lin4 = torch.nn.Linear(4 * hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.elu(self.conv1(x, edge_index) + self.lin1(x))\n",
    "        x = F.elu(self.conv2(x, edge_index) + self.lin2(x))\n",
    "        x = F.elu(self.conv3(x, edge_index) + self.lin3(x))\n",
    "        #x = F.elu(self.conv5(x, edge_index) + self.lin5(x))\n",
    "        x = self.conv4(x, edge_index) + self.lin4(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv, GINConv, GCNConv, GATv2Conv, ARMAConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(train_dataset.num_features, hidden_channels)\n",
    "        self.conv2 = GATv2Conv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GATv2Conv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = ARMAConv(hidden_channels, hidden_channels)\n",
    "        self.lin1 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.lin = torch.nn.Linear(hidden_channels, 1)\n",
    "        self.conv_last = GCNConv(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.lin2(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv4(x, edge_index)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch.nn.modules.loss import _Loss\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "class CustomLossFunction(torch.nn.BCEWithLogitsLoss):\n",
    "    \"\"\"Custom loss function based on BCEWithLogitsLoss\"\"\"\n",
    "    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
    "        tens = torch.split(target, input.size(dim=0))[0]\n",
    "        loss = F.binary_cross_entropy_with_logits(input, tens, self.weight, pos_weight=self.pos_weight, reduction=self.reduction)\n",
    "\n",
    "        for tens in torch.split(target, input.size(dim=0)):\n",
    "            new = F.binary_cross_entropy_with_logits(input, tens, self.weight, pos_weight=self.pos_weight, reduction=self.reduction)\n",
    "            if new.item() < loss.item():\n",
    "                loss = new\n",
    "        return loss\n",
    "\n",
    "\n",
    "class UnsupervisedLossFunction(torch.nn.BCEWithLogitsLoss):\n",
    "\n",
    "    def forward(self, A, candidate, target_value) -> Tensor:\n",
    "        n = len(candidate)\n",
    "        loss = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "        # loss for MIDS size\n",
    "        loss = torch.add(loss, abs(sum(candidate) - target_value))\n",
    "\n",
    "        # loss for domination\n",
    "        for el in ((A + np.eye(n)) @ candidate):\n",
    "            if el < 1:\n",
    "                loss = torch.add(loss, 1/n) # devide by factor (size)\n",
    "\n",
    "        # loss for independance\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                if candidate[i] and candidate[j] and A[i,j]:\n",
    "                    loss = torch.add(loss, 1/n) # devide by factsor (size)\n",
    "        return loss\n",
    "\n",
    "def check_accuracy_unsupervised(data, out, criterion):\n",
    "    pred = torch.where(out > 0, 1.0, 0.0)\n",
    "    num_nodes = data.num_nodes\n",
    "    adjacency = SparseTensor(row=data.edge_index[0], col=data.edge_index[1], sparse_sizes=(num_nodes, num_nodes))\n",
    "    adjacency_matrix = adjacency.to_dense()\n",
    "    return criterion(adjacency_matrix.cpu().numpy(),\n",
    "                                              out.detach().cpu().numpy(),\n",
    "                                              sum(torch.split(data.y, data.num_nodes)[0].cpu().numpy()))\n",
    "\n",
    "\n",
    "'''\n",
    "def UnsupervisedLossFunction(A, candidate, target_value):\n",
    "    n = len(candidate)\n",
    "    loss = 0\n",
    "\n",
    "    # loss for MIDS size\n",
    "    loss += abs(sum(candidate) - target_value)\n",
    "\n",
    "    # loss for domination\n",
    "    for el in ((A + np.eye(n)) @ candidate):\n",
    "        if el < 1:\n",
    "            loss += 1/n # devide by factor (size)\n",
    "\n",
    "    # loss for independance\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if candidate[i] and candidate[j] and A[i,j]:\n",
    "                loss += 1/n # devide by factsor (size)\n",
    "\n",
    "    return torch.tensor(loss, requires_grad=True)\n",
    "'''\n",
    "\n",
    "def check_MIDS(A, candidate, target_value):\n",
    "    n = len(candidate)\n",
    "\n",
    "    # Candidate set is larger than MIDS size\n",
    "    if int(sum(candidate)) > int(target_value):\n",
    "        return False\n",
    "\n",
    "    # Candidate set is not dominating\n",
    "    if not all((A + np.eye(n)) @ candidate >= 1):\n",
    "        return False\n",
    "\n",
    "    # Candidate set is not independent\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if candidate[i] and candidate[j] and A[i,j]:\n",
    "                return False\n",
    "\n",
    "    #This case should never happen\n",
    "    if int(sum(candidate)) < int(target_value):\n",
    "        print(f\"Somehow we found an even smaller MIDS: {sum(candidate)}, {target_value}\")\n",
    "        pass\n",
    "\n",
    "    return True\n",
    "\n",
    "def generate_model(hidden_channels):\n",
    "    \"\"\"Generate a Neural Network model based on the architecture and hyperparameters.\"\"\"\n",
    "    model = Net(hidden_channels=hidden_channels).to(device)\n",
    "    return model\n",
    "\n",
    "def generate_optimizer(model, optimizer, lr):\n",
    "    \"\"\"Generate optimizer object based on the model and hyperparameters.\"\"\"\n",
    "    if optimizer == \"adam\":\n",
    "        return torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise ValueError(\"Only Adam optimizer is currently supported.\")\n",
    "\n",
    "\n",
    "def training_pass(model, batch, optimizer, criterion):\n",
    "    \"\"\"Perofrm a single training pass over the batch.\"\"\"\n",
    "    data = batch.to(device)\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out = model(data.x.type(torch.FloatTensor).to(device), data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "    loss = criterion(out, data.y)  # Compute the loss.\n",
    "    #loss = check_accuracy_unsupervised(batch, out, criterion)\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    return loss.item()\n",
    "\n",
    "def check_accuracy(data, out):\n",
    "    pred = torch.where(out > 0, 1.0, 0.0)\n",
    "    num_nodes = data.num_nodes\n",
    "    adjacency = SparseTensor(row=data.edge_index[0], col=data.edge_index[1], sparse_sizes=(num_nodes, num_nodes))\n",
    "    adjacency_matrix = adjacency.to_dense()\n",
    "    return int(check_MIDS(adjacency_matrix.cpu().numpy(),\n",
    "                          pred.cpu().numpy(),\n",
    "                          sum(torch.split(data.y, num_nodes)[0].cpu().numpy())\n",
    "                          )\n",
    "               )\n",
    "\n",
    "\n",
    "def testing_pass(model, batch, criterion):\n",
    "    \"\"\"Perform a single testing pass over the batch.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        data = batch.to(device)\n",
    "        out = model(data.x.type(torch.FloatTensor).to(device), data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y).item()  # Compute the loss.\n",
    "        #loss = check_accuracy_unsupervised(data, out, criterion).item()\n",
    "        correct = 0\n",
    "        start_slice = 0\n",
    "        for i in range(len(data)):\n",
    "            d = data[i]\n",
    "            correct += check_accuracy(d, out[start_slice:start_slice+d.y.shape[0]])\n",
    "            start_slice += d.y.shape[0]\n",
    "    return loss, correct\n",
    "\n",
    "\n",
    "def do_train(model, data, optimizer, criterion):\n",
    "    \"\"\"Train the model on individual batches or the entire dataset.\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    if isinstance(data, DataLoader):\n",
    "        avg_loss = 0\n",
    "        for batch in data:  # Iterate in batches over the training dataset.\n",
    "            avg_loss += training_pass(model, batch, optimizer, criterion)\n",
    "        loss = avg_loss / len(data)\n",
    "    elif isinstance(data, Data):\n",
    "        loss = training_pass(model, data, optimizer, criterion)\n",
    "    else:\n",
    "        raise ValueError(\"Data must be a DataLoader or a Batch object.\")\n",
    "    return loss\n",
    "\n",
    "\n",
    "def do_test(model, data, criterion):\n",
    "    \"\"\"Test the model on individual batches or the entire dataset.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    if isinstance(data, DataLoader):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        losses = 0\n",
    "        for batch in data:\n",
    "            loss, corr = testing_pass(model, batch, criterion)\n",
    "            losses += loss\n",
    "            correct += corr\n",
    "            total += len(batch)\n",
    "        total_loss = losses / len(data)\n",
    "        accuracy = correct / total * 100\n",
    "    elif isinstance(data, Data):\n",
    "        total_loss, correct = testing_pass(model, data, criterion)\n",
    "        accuracy = correct / len(data) * 100\n",
    "    else:\n",
    "        raise ValueError(\"Data must be a DataLoader or a Batch object.\")\n",
    "\n",
    "    return total_loss, accuracy\n",
    "\n",
    "\n",
    "def train(model, optimizer, criterion, num_epochs=200, is_sweep=False):\n",
    "    # GLOBALS: device, dataset, train_dataset, train_dataset\n",
    "\n",
    "    # Prepare for training.\n",
    "    train_losses = np.zeros(num_epochs)\n",
    "    test_losses = np.zeros(num_epochs)\n",
    "\n",
    "    # Start the training loop with timer.\n",
    "    training_timer = codetiming.Timer(logger=None)\n",
    "    epoch_timer = codetiming.Timer(logger=None)\n",
    "    training_timer.start()\n",
    "    epoch_timer.start()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Perform one pass over the training set and then test on both sets.\n",
    "        train_loss = do_train(model, train_data_obj, optimizer, criterion)\n",
    "        if epoch % 100 == 0 or epoch == num_epochs:\n",
    "            train_loss, train_acc = do_test(model, train_data_obj, criterion)\n",
    "            test_loss, test_acc = do_test(model, test_data_obj, criterion)\n",
    "\n",
    "        # Store the losses.\n",
    "        train_losses[epoch - 1] = train_loss\n",
    "        test_losses[epoch - 1] = test_loss\n",
    "\n",
    "        dur = epoch_timer.stop()\n",
    "        print(\n",
    "            f\"Epoch: {epoch:03d}, \"\n",
    "            f\"Train Loss: {train_loss:.4f}, \"\n",
    "            f\"Test Loss: {test_loss:.4f}, \"\n",
    "            f\"Train Accuracy: {train_acc:.4f}, \"\n",
    "            f\"Test Accuracy: {test_acc:.4f}, \"\n",
    "            f\"Avg. duration: {dur:.4f} s\"\n",
    "        )\n",
    "        wandb.log({\"train_loss\": train_loss, \"test_loss\": test_loss,\n",
    "                    \"train_acc\": train_acc, \"test_acc\": test_acc,\n",
    "                    \"train_loss_scaled\": train_loss/0.8, \"test_loss_scaled\": test_loss/0.2,\n",
    "                    \"epoch_time\": dur})\n",
    "        epoch_timer.start()\n",
    "    epoch_timer.stop()\n",
    "    duration = training_timer.stop()\n",
    "\n",
    "    results = {\"train_losses\": train_losses, \"test_losses\": test_losses, \"duration\": duration}\n",
    "    return results, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from utils import create_graph_wandb, extract_graphs_from_batch, graphs_to_tuple\n",
    "\n",
    "\n",
    "def plot_training_curves(num_epochs, train_losses, test_losses, criterion):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, num_epochs + 1)), y=train_losses, mode=\"lines\", name=\"Train Loss\"))\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, num_epochs + 1)), y=test_losses, mode=\"lines\", name=\"Test Loss\"))\n",
    "    fig.update_layout(title=\"Training and Test Loss\", xaxis_title=\"Epoch\", yaxis_title=criterion)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def evaluate(model, plot_graphs=False, is_sweep=False):\n",
    "    # GLOBALS: dataset_config, train_loader, test_loader\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Evaluate the model on the test set.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:  # Iterate in batches over the training/test dataset.\n",
    "            # Make predictions.\n",
    "            data = data.to(device)\n",
    "            out = model(data.x.type(torch.FloatTensor).to(device), data.edge_index)\n",
    "            predictions = out.cpu().numpy().squeeze()\n",
    "            ground_truth = data.y.cpu().numpy()\n",
    "\n",
    "            # Extract graphs and create visualizations.\n",
    "            nx_graphs = extract_graphs_from_batch(data)\n",
    "            graphs, node_nums, edge_nums = zip(*graphs_to_tuple(nx_graphs))\n",
    "            # FIXME: This is the only way to parallelize in Jupyter but runs out of memory.\n",
    "            # with concurrent.futures.ProcessPoolExecutor(4) as executor:\n",
    "            #     graph_visuals = executor.map(create_graph_wandb, nx_graphs, chunksize=10)\n",
    "            if plot_graphs:\n",
    "                graph_visuals = [create_graph_wandb(g) for g in nx_graphs]\n",
    "            else:\n",
    "                graph_visuals = [\"N/A\"] * len(nx_graphs)\n",
    "\n",
    "            # Store to pandas DataFrame.\n",
    "            df = pd.concat(\n",
    "                [\n",
    "                    df,\n",
    "                    pd.DataFrame(\n",
    "                        {\n",
    "                            \"GraphVis\": graph_visuals,\n",
    "                            \"Graph\": graphs,\n",
    "                            \"Nodes\": node_nums,\n",
    "                            \"Edges\": edge_nums,\n",
    "                            \"True\": ground_truth,\n",
    "                            \"Predicted\": predictions,\n",
    "                        }\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    # Calculate the statistics.\n",
    "    df[\"Error\"] = df[\"True\"] - df[\"Predicted\"]\n",
    "    df[\"Error %\"] = 100 * df[\"Error\"] / df[\"True\"]\n",
    "    df[\"abs(Error)\"] = np.abs(df[\"Error\"])\n",
    "    err_mean = np.mean(df[\"abs(Error)\"])\n",
    "    err_stddev = np.std(df[\"abs(Error)\"])\n",
    "\n",
    "    # Create a W&B table.\n",
    "    table = wandb.Table(dataframe=df)\n",
    "\n",
    "    # Print and plot.\n",
    "    # df = df.sort_values(by=\"abs(Error)\")\n",
    "    fig_abs_err = px.histogram(df, x=\"Error\")\n",
    "    fig_rel_err = px.histogram(df, x=\"Error %\")\n",
    "\n",
    "    if not is_sweep:\n",
    "        print(f\"Mean error: {err_mean:.4f}\\n\" f\"Std. dev.: {err_stddev:.4f}\\n\")\n",
    "        fig_abs_err.show()\n",
    "        fig_rel_err.show()\n",
    "        df = df.sort_values(by=\"Nodes\")\n",
    "        print(df)\n",
    "\n",
    "    results = {\n",
    "        \"mean_err\": err_mean,\n",
    "        \"stddev_err\": err_stddev,\n",
    "        \"fig_abs_err\": fig_abs_err,\n",
    "        \"fig_rel_err\": fig_rel_err,\n",
    "        \"table\": table,\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "def main(config=None, skip_evaluation=False):\n",
    "    # GLOBALS: device, dataset, train_dataset, test_dataset\n",
    "\n",
    "    is_sweep = config is None\n",
    "\n",
    "    # Set up the run\n",
    "    run = wandb.init(mode=\"disabled\", project=\"GNN_MIDS\", config=config)\n",
    "    config = wandb.config\n",
    "    if is_sweep:\n",
    "        print(f\"Running sweep with config: {config}...\")\n",
    "\n",
    "    # Set up the model, optimizer, and criterion.\n",
    "    model = generate_model(config[\"hidden_channels\"])\n",
    "    optimizer = generate_optimizer(model, config[\"optimizer\"], config[\"learning_rate\"])\n",
    "    criterion = CustomLossFunction()\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    #criterion = UnsupervisedLossFunction()\n",
    "\n",
    "    # Run training.\n",
    "    print(\"Training started\")\n",
    "    train_results, model = train(model, optimizer, criterion, config[\"epochs\"], is_sweep=is_sweep)\n",
    "    run.summary[\"best_train_loss\"] = min(train_results[\"train_losses\"])\n",
    "    run.summary[\"best_test_loss\"] = min(train_results[\"test_losses\"])\n",
    "    run.summary[\"duration\"] = train_results[\"duration\"]\n",
    "    # if not is_sweep:\n",
    "    #     plot_training_curves(\n",
    "    #         config[\"epochs\"], train_results[\"train_losses\"], train_results[\"test_losses\"], type(criterion).__name__\n",
    "    #     )\n",
    "\n",
    "    return run, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_config = {\n",
    "    \"seed\": seed,\n",
    "    \"dataset\": dataset_config,\n",
    "    \"hidden_channels\": 128,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"learning_rate\": 0.0003,\n",
    "    \"epochs\": 3200,\n",
    "    \"layers\": \"4\",\n",
    "    \"architecture\": \"GAT\",\n",
    "    \"batch\": \"yes\",\n",
    "    \"solutions\": \"one\",\n",
    "}\n",
    "run, model = main(global_config)\n",
    "#torch.save(model, '/home/jovyan/models/42000_lr-0001_lf-multi_8f_lay-4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/home/jovyan/models/encoded_solutions.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/home/jovyan/models/encoded_solutions.pth')\n",
    "\n",
    "\n",
    "def get_highest_n_positions(array, n):\n",
    "    # Convert the array to a numpy array for easy manipulation\n",
    "    array = np.array(array)\n",
    "\n",
    "    # Get the indices of the n highest values\n",
    "    highest_indices = np.argsort(array)[-n:]\n",
    "\n",
    "    # Create a new array of zeros with the same length\n",
    "    result = np.zeros_like(array)\n",
    "\n",
    "    # Set ones at the positions of the highest values\n",
    "    result[highest_indices] = 1\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def check_MIDS_test(A, candidate, target_value, out):\n",
    "    n = len(candidate)\n",
    "    loss = 0\n",
    "    # Candidate set is not minimal\n",
    "    if int(sum(candidate)) > int(target_value):\n",
    "        return False# not minimal\n",
    "\n",
    "    # Candidate set is not dominating and independent\n",
    "    if not all((A + np.eye(n)) @ candidate >= 1): # domination\n",
    "        return False\n",
    "\n",
    "    for el in ((A + np.eye(n)) @ candidate):\n",
    "        if el < 1:\n",
    "            loss += 1\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if candidate[i] and candidate[j] and A[i,j]: # independancy\n",
    "                return False # TODO: zbrajati koliko ih ima koji ispunjavaju uvijet - iz tog dobiti mjeru (LOSS) nezavisnosti\n",
    "\n",
    "    if int(sum(candidate)) < int(target_value):\n",
    "        print(f\"Somehow we found an even smaller MIDS: {sum(candidate)}, {target_value}\")\n",
    "        pass\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    flag = 0\n",
    "    correct = 0\n",
    "    correct_checkMIDS = 0\n",
    "    correct_checkMIDS_new = 0\n",
    "    not_min = 0\n",
    "    one_less = 0\n",
    "    off_by_one = 0\n",
    "    for i, data in enumerate(loader):\n",
    "        for idx in range(data.num_graphs):\n",
    "            out = model(data.get_example(idx).x.type(torch.FloatTensor).to(device), data.get_example(idx).edge_index.to(device), data.batch)\n",
    "            #print(out)\n",
    "            pred = torch.where(out > 0, 1.0, 0.0)\n",
    "            #print(data.edge_index)\n",
    "\n",
    "            #correct += int(torch.equal(pred.to(device),data.get_example(idx).y.type(torch.LongTensor).to(device)))  # Derive ratio of correct predictions.\n",
    "\n",
    "            num_nodes = data.get_example(idx).num_nodes\n",
    "            adjacency = SparseTensor(row=data.get_example(idx).edge_index[0], col=data.get_example(idx).edge_index[1], sparse_sizes=(num_nodes, num_nodes))\n",
    "            adjacency_matrix = adjacency.to_dense()\n",
    "            correct = check_MIDS_test(adjacency_matrix.numpy(),\n",
    "                                pred.cpu().numpy(),\n",
    "                                sum(torch.split(data.get_example(idx).y, data.num_nodes)[0].cpu().numpy()), out)\n",
    "            correct_new = check_MIDS_test(adjacency_matrix.numpy(),\n",
    "                                get_highest_n_positions(out.cpu(), int(sum(torch.split(data.get_example(idx).y, data.num_nodes)[0].cpu().numpy()))),\n",
    "                                sum(torch.split(data.get_example(idx).y, data.num_nodes)[0].cpu().numpy()), out)\n",
    "\n",
    "            correct_checkMIDS_new += int(correct_new)\n",
    "            if correct == True:\n",
    "                correct_checkMIDS += int(correct)\n",
    "            else:\n",
    "                if correct == -5: # not minimal\n",
    "                    not_min += 1\n",
    "                if correct == -4: # 1 less\n",
    "                    one_less += 1\n",
    "                    #print(get_highest_n_positions(out.cpu(), int(sum(pred).cpu())+1))\n",
    "                    off_by_one += check_MIDS_test(adjacency_matrix.numpy(),\n",
    "                                get_highest_n_positions(out.cpu(), int(sum(pred).cpu())+1),\n",
    "                                sum(torch.split(data.get_example(idx).y, data.num_nodes)[0].cpu().numpy()), out)\n",
    "\n",
    "\n",
    "    #print(f'{correct_checkMIDS/len(loader.dataset)*100}%')\n",
    "    print(correct_checkMIDS)\n",
    "    print(len(loader.dataset))\n",
    "    return correct_checkMIDS/len(loader.dataset)*100, correct_checkMIDS_new/len(loader.dataset)*100#, not_min/len(loader.dataset)*100, one_less/len(loader.dataset)*100, off_by_one/len(loader.dataset)*100\n",
    "\n",
    "\n",
    "root = \"/home/jovyan/MIDS-GNN/Dataset\"\n",
    "graphs_loader = GraphDataset(selection=selected_graph_sizes)\n",
    "dataset = MIDSdataset(root, graphs_loader)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "test(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_pred(data, pred):\n",
    "    G = pygUtils.to_networkx(data, to_undirected=True)\n",
    "    nx.draw(G, with_labels=True, node_color=pred, cmap=matplotlib.colormaps[\"bwr\"])\n",
    "    plt.show()\n",
    "    print(f'prediction: {pred}')\n",
    "    print(f'data.y:     {torch.split(data.y, data.num_nodes)}')\n",
    "\n",
    "for i in range(50):\n",
    "    d = dataset[i]\n",
    "    out = model(d.x.type(torch.FloatTensor).to(device), d.edge_index.to(device))\n",
    "    pred = torch.where(out > 0, 1.0, 0.0)\n",
    "    num_nodes = d.num_nodes\n",
    "    adjacency = SparseTensor(row=d.edge_index[0], col=d.edge_index[1], sparse_sizes=(num_nodes, num_nodes))\n",
    "    adjacency_matrix = adjacency.to_dense()\n",
    "    correct = check_MIDS_test(adjacency_matrix.numpy(),\n",
    "                                get_highest_n_positions(out.cpu(), int(sum(torch.split(d.y, d.num_nodes)[0].cpu()))),\n",
    "                                sum(torch.split(d.y, data.num_nodes)[0].cpu()), out)\n",
    "\n",
    "    if correct == False:\n",
    "        examine_pred(d.cpu(), pred.cpu())\n",
    "        print(model(d.x.type(torch.FloatTensor).to(device), d.edge_index.to(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W&B sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%env WANDB_SILENT=True\n",
    "\n",
    "import time\n",
    "time.sleep(2)\n",
    "\n",
    "# TODO: How to include seed and dataset configuration?\n",
    "\n",
    "full_sweep_configuration = {\n",
    "    \"name\": \"full_first_sweep\",\n",
    "    \"method\": \"grid\",  # grid, random or Bayesian\n",
    "    \"metric\": {\"goal\": \"minimize\", \"name\": \"test_loss\"},\n",
    "    \"parameters\": {\n",
    "        \"architecture\": {\"value\": \"GAT\"},\n",
    "        \"hidden_channels\": {\"values\": [256]},\n",
    "        \"optimizer\": {\"value\": \"adam\"},\n",
    "        \"learning_rate\": {\"values\": [0.0002, 0.0004, 0.0006]},\n",
    "        \"epochs\": {\"value\": 150}\n",
    "    },\n",
    "    \"early_terminate\": {\n",
    "        \"type\": \"hyperband\",\n",
    "        \"eta\": 3,\n",
    "        \"min_iter\": 150\n",
    "    }\n",
    "}\n",
    "\n",
    "test_sweep_configuration = {\n",
    "    \"name\": \"test_sweep\",\n",
    "    \"method\": \"grid\",  # grid, random or Bayesian\n",
    "    \"metric\": {\"goal\": \"minimize\", \"name\": \"test_loss\"},\n",
    "    \"parameters\": {\n",
    "        \"architecture\": {\"values\": [\"GCN\", \"GAT\"]},\n",
    "        \"hidden_channels\": {\"values\": [8, 16]},\n",
    "        \"num_layers\": {\"values\": [2, 5]},\n",
    "        \"optimizer\": {\"value\": \"adam\"},\n",
    "        \"learning_rate\": {\"values\": [0.01, 0.001]},\n",
    "        \"epochs\": {\"value\": 1000}\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=full_sweep_configuration, project=\"GNN_MIDS\")\n",
    "\n",
    "wandb.agent(sweep_id, function=main, count=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, function=main, count=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the W&B run.\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housekeeping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"model.pth\")\n",
    "# print(\"Saved PyTorch Model State to model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions with loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NeuralNetwork().to(device)\n",
    "# model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "# classes = [\n",
    "#     \"T-shirt/top\",\n",
    "#     \"Trouser\",\n",
    "#     \"Pullover\",\n",
    "#     \"Dress\",\n",
    "#     \"Coat\",\n",
    "#     \"Sandal\",\n",
    "#     \"Shirt\",\n",
    "#     \"Sneaker\",\n",
    "#     \"Bag\",\n",
    "#     \"Ankle boot\",\n",
    "# ]\n",
    "\n",
    "# model.eval()\n",
    "# x, y = test_data[0][0], test_data[0][1]\n",
    "# with torch.no_grad():\n",
    "#     x = x.to(device)\n",
    "#     pred = model(x)\n",
    "#     predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "#     print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional W&B APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api = wandb.Api()\n",
    "\n",
    "# # Access attributes directly from the run object\n",
    "# # or from the W&B App\n",
    "# username = \"marko-krizmancic\"\n",
    "# project = \"gnn_fiedler_approx\"\n",
    "# run_id = [\"nrcdc1y4\", \"11l94b1a\", \"ptj7b0vx\"]\n",
    "\n",
    "# for id in run_id:\n",
    "#     run = api.run(f\"{username}/{project}/{id}\")\n",
    "#     run.config[\"model\"] = \"GCN\"\n",
    "#     run.update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
