{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import wandb\n",
    "api = wandb.Api()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download all runs from a sweep and store data in a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels all\n",
    "sweep_id = \"12qeci5o\"\n",
    "full_csv = \"labels_all_full.csv\"\n",
    "best_csv = \"labels_all_best.csv\"\n",
    "plot_title = \"Multi Labels\"\n",
    "\n",
    "# Labels single\n",
    "# sweep_id = \"4yorg0e3\"\n",
    "# full_csv = \"labels_single_full.csv\"\n",
    "# best_csv = \"labels_single_best.csv\"\n",
    "# plot_title = \"Single Label\"\n",
    "\n",
    "# Combined best\n",
    "combined_csv = \"combined_best.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project is specified by <entity/project-name>\n",
    "sweep = api.sweep(f\"LARICS-GNN/MIDS-GNN/{sweep_id}\")  # labels_all\n",
    "runs = sweep.runs\n",
    "\n",
    "summary_fields = ['train_loss', 'val_loss', 'eval_loss', 'best_train_loss', 'best_val_loss', 'train_acc', 'val_acc', 'eval_accuracy', 'best_train_accuracy', 'best_val_accuracy', 'mean_error', 'mean_abs_error', 'std_error', 'duration']\n",
    "config_fields = ['jk', 'activation', 'gnn_layers', 'architecture', 'learning_rate', 'hidden_channels', 'selected_extra_feature']\n",
    "\n",
    "records = []\n",
    "for run in runs:\n",
    "    summary = {k: run.summary[k] for k in summary_fields}\n",
    "    config = {k: run.config[k] for k in config_fields}\n",
    "    config[\"target\"] = run.config[\"dataset\"][\"target\"]\n",
    "    info = {\"name\": run.name, \"id\": run.id}\n",
    "    records.append({**info, **config, **summary})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = pathlib.Path().cwd() / \"Results\"  # For Jupyter notebook.\n",
    "\n",
    "runs_df = pd.DataFrame.from_records(records)\n",
    "runs_df = runs_df.fillna('none')\n",
    "runs_df.to_csv(save_path / full_csv)\n",
    "\n",
    "runs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from all runs, select the best, and store it in a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame from the CSV file\n",
    "loaded_df = pd.read_csv(save_path / full_csv, index_col=0)\n",
    "\n",
    "# Find the rows with the smallest 'best_val_loss' for each unique combination of 'architecture' and 'selected_extra_feature'\n",
    "best_rows = loaded_df.loc[loaded_df.groupby(['architecture', 'selected_extra_feature'])['best_val_loss'].idxmin()]\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "best_rows.to_csv(save_path / best_csv)\n",
    "\n",
    "best_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine csv files from two sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files into DataFrames\n",
    "labels_all_best_df = pd.read_csv(save_path / \"labels_all_best.csv\", index_col=0)\n",
    "labels_single_best_df = pd.read_csv(save_path / \"labels_single_best.csv\", index_col=0)\n",
    "\n",
    "# Combine the DataFrames\n",
    "combined_df = pd.concat([labels_all_best_df, labels_single_best_df])\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "combined_df.to_csv(save_path / combined_csv)\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a summarized results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the combined_best.csv into a dataframe\n",
    "combined_best_df = pd.read_csv(save_path / combined_csv, index_col=0)\n",
    "\n",
    "# Create a new column with the formatted values\n",
    "combined_best_df['formatted_accuracy'] = combined_best_df.apply(\n",
    "    lambda row: f\"{row['best_val_accuracy']:.2f} $|$ {row['eval_accuracy']:.2f}\", axis=1\n",
    ")\n",
    "\n",
    "# Pivot the dataframe to create the desired table\n",
    "pivot_table = combined_best_df.pivot_table(\n",
    "    index='architecture',\n",
    "    columns=['selected_extra_feature', 'target'],\n",
    "    values='formatted_accuracy',\n",
    "    aggfunc=\"first\"\n",
    ")\n",
    "\n",
    "# Reorder the columns to match the desired structure\n",
    "pivot_table = pivot_table.reindex(columns=pd.MultiIndex.from_product(\n",
    "    [combined_best_df['selected_extra_feature'].unique(), ['true_labels_all_padded', 'true_labels_single']],\n",
    "    names=['Feature', 'Target']))\n",
    "\n",
    "# Rename the columns for latex\n",
    "pivot_table.columns = pivot_table.columns.set_levels(\n",
    "    ['Noisy Probability' if x == 'noisy_probability' else\n",
    "     'True Probability' if x == 'true_probability' else\n",
    "     'Predicted Probability' if x == 'predicted_probability' else\n",
    "     'None' if x == 'none' else x\n",
    "     for x in pivot_table.columns.levels[0]], level=0)\n",
    "\n",
    "pivot_table.columns = pivot_table.columns.set_levels(\n",
    "    ['Multi' if x == 'true_labels_all_padded' else\n",
    "     'Single' if x == 'true_labels_single' else x\n",
    "     for x in pivot_table.columns.levels[1]], level=1)\n",
    "\n",
    "# Sort the table with custom order\n",
    "pivot_table = pivot_table.reindex([\"MLP\", \"GCN\", \"GIN\", \"GraphSAGE\", \"GAT\", \"GATLinNet\"])\n",
    "pivot_table = pivot_table.reindex(columns=[\"None\", \"Noisy Probability\", \"Predicted Probability\", \"True Probability\"], level=0)\n",
    "\n",
    "# Generate LaTeX code for the table\n",
    "latex_code = pivot_table.to_latex(column_format=\"l|cccccccc\", multicolumn=True, multicolumn_format=\"c\", multirow=True, escape=False)\n",
    "\n",
    "print(latex_code)\n",
    "\n",
    "pivot_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw bar plots for the entire results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "\n",
    "# Load the dataframe from the CSV file\n",
    "runs_df = pd.read_csv(save_path / full_csv, index_col=0)\n",
    "\n",
    "# List of config parameters to plot\n",
    "config_params = ['jk', 'activation', 'gnn_layers', 'architecture', 'learning_rate', 'hidden_channels', 'selected_extra_feature']\n",
    "\n",
    "# Target value to plot\n",
    "target_value = 'eval_accuracy'\n",
    "\n",
    "# Create a subplot grid\n",
    "fig = sp.make_subplots(rows=3, cols=3, subplot_titles=config_params)\n",
    "\n",
    "# Add box plots to the subplots\n",
    "for i, param in enumerate(config_params):\n",
    "    row = i // 3 + 1\n",
    "    col = i % 3 + 1\n",
    "    box_fig = px.box(runs_df, x=param, y=target_value, color=param)\n",
    "    for trace in box_fig['data']:\n",
    "        fig.add_trace(trace, row=row, col=col)\n",
    "\n",
    "# Update layout\n",
    "fig.update_xaxes(type='category')\n",
    "fig.update_layout(title_text=f'{target_value} for {plot_title}', showlegend=False, height=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download saved best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load the DataFrame with best results.\n",
    "best_df = pd.read_csv(save_path / best_csv, index_col=0)\n",
    "\n",
    "predicted_probability_df = best_df[best_df['selected_extra_feature'] == 'predicted_probability']\n",
    "\n",
    "# Base path for the files\n",
    "base_path = \"mkrizman@login-gpu.hpc.srce.hr:/storage/home/mkrizman/MIDS-GNN/sweep_all_labels/Models/\"\n",
    "\n",
    "# Iterate over the rows in the DataFrame\n",
    "for index, row in predicted_probability_df.iterrows():\n",
    "    file_name = f\"{row['id']}_best_model.pth\"\n",
    "    source_path = base_path + file_name\n",
    "    destination_path = pathlib.Path().cwd() / \"Models\"\n",
    "    destination_path = str(destination_path.resolve())\n",
    "\n",
    "    # Use scp to copy the file\n",
    "    os.system(f\"scp {source_path} {destination_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
